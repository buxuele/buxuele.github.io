{"meta":{"title":"Legend","subtitle":"jsut rolling","description":"test something","author":"buxuele","url":"http://buxuele.github.io"},"pages":[{"title":"about_me","date":"2018-04-21T03:44:01.000Z","updated":"2018-04-21T10:33:24.992Z","comments":true,"path":"about-me/index.html","permalink":"http://buxuele.github.io/about-me/index.html","excerpt":"","text":"If there’s anything you want to tell me ,please send:baogebuxuele@163.com"},{"title":"tags","date":"2018-04-21T07:11:40.000Z","updated":"2018-04-21T07:11:40.236Z","comments":true,"path":"tags/index.html","permalink":"http://buxuele.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"523readingNote04","slug":"523readingNote04","date":"2018-05-23T16:05:28.000Z","updated":"2018-05-23T16:08:25.173Z","comments":true,"path":"2018/05/24/523readingNote04/","link":"","permalink":"http://buxuele.github.io/2018/05/24/523readingNote04/","excerpt":"","text":"近期读书笔记04包法利夫人读起来感觉很好，鼠疫简直读不下去。pictest 123456781. 释迦牟尼：这种不注意周围环境的心理往往足以遏制改进物质生活的愿望，而改进物质生活的愿望，会给科学知识的进步带来推动力。2. 某作家：每天她都会写一点，不报希望，亦无悲观。3. 一根雪茄和泽西岛历历在目让你心旷神怡。4. 他写道：没有人曾经更多体尝莫测的命运，我十三次发财致富而又沦为赤贫。5. 包法利夫人：可是对着一切，她渐渐厌倦了，却又不承认，只是靠习惯和虚荣心，才得以支撑下来。6. 包法利夫人：她未尝不想下楼与女佣人聊聊天，可是碍着面子，又打消那念头。7. 包法利夫人：她很懊恼，但越懊恼，欲望越强烈。8. 鼠疫中某医生：他有一副好脾气，总是面带笑容。他似乎对所有正常的娱乐活动都感兴趣，但又不沉迷其中。","categories":[],"tags":[]},{"title":"523gevent","slug":"523gevent","date":"2018-05-23T15:50:55.000Z","updated":"2018-05-23T16:02:08.219Z","comments":true,"path":"2018/05/23/523gevent/","link":"","permalink":"http://buxuele.github.io/2018/05/23/523gevent/","excerpt":"","text":"以后 主要，还是多用 gevent。稳定，持续，最重要的是：速度快。个人感觉是比 自带的aiohttp + asyncio 要好用，且快。123456789101112131415161718192021222324252627&apos;&apos;&apos;gevent downlaod gevent is much faster than multiprocessing.dummy &apos;&apos;&apos;import requests import re import time import os import gevent from gevent import monkey monkey.patch_all()def get_page(url): cont = requests.get(url).content with open(url.split(&apos;/&apos;)[-1], &apos;wb&apos;) as f: f.write(cont) urls = [u.strip() for u in open(&apos;us.txt&apos;, &apos;r&apos;)]tasks = [gevent.spawn(get_page, url) for url in urls]gevent.joinall(tasks)","categories":[],"tags":[]},{"title":"520aiohttp_bing01","slug":"520aiohttp-bing01","date":"2018-05-20T14:21:13.000Z","updated":"2018-05-20T14:29:23.936Z","comments":true,"path":"2018/05/20/520aiohttp-bing01/","link":"","permalink":"http://buxuele.github.io/2018/05/20/520aiohttp-bing01/","excerpt":"","text":"用aiohttp 下载 bing壁纸（目前只能下7张）。也算是异步的一次小小的尝试了。 1234567891011121314151617181920212223242526272829303132333435363738394041# -*- coding: utf-8 -*-# info: fanchuang 2018/5/20 19:35# 目的: import osimport timeimport reimport clickimport asyncioimport aiohttp# url = &apos;https://cn.bing.com/HPImageArchive.aspx?format=js&amp;idx=0&amp;n=1&amp;nc=1526816726462&amp;pid=hp&apos;TODAY = time.strftime(&apos;%Y-%m-%d&apos;, time.localtime(time.time()))PATTERN = re.compile(r&apos;&apos;)async def getPic(url): async with aiohttp.ClientSession() as session: async with session.get(url) as resp: json_file = await resp.json() data = json_file[&apos;images&apos;] for d in data: pic_url = &apos;https://cn.bing.com&apos; + d[&apos;url&apos;] click.echo(pic_url) async with session.get(pic_url) as res: with open(pic_url.split(&apos;/&apos;)[-1], &apos;wb&apos;) as f: cont = await res.read() f.write(cont)if __name__ == &apos;__main__&apos;: os.makedirs(&apos;必应壁纸&apos;, exist_ok=True) os.chdir(&apos;必应壁纸&apos;) loop = asyncio.get_event_loop() urls = [&apos;https://cn.bing.com/HPImageArchive.aspx?format=js&amp;idx=%d&amp;n=1&amp;nc=1526823492560&amp;pid=hp&apos; % x for x in range(7)] tasks = [getPic(url) for url in urls] loop.run_until_complete(asyncio.gather(*tasks)) # print(TODAY)","categories":[],"tags":[]},{"title":"510low","slug":"510low","date":"2018-05-10T12:32:55.000Z","updated":"2018-05-10T16:31:53.383Z","comments":true,"path":"2018/05/10/510low/","link":"","permalink":"http://buxuele.github.io/2018/05/10/510low/","excerpt":"","text":"阳台上的瓜苗啊。你注定是命途艰难啊。另外，下载，合并ts文件（初步版本）。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556# -*- coding: utf-8 -*-# author: fanchuangwater Administrator# time: 2018/5/10 23:59# about: # 当前这是第一步，还有许多是需要完善的。针对一个站点来批量操作是下一步的任务。import reimport subprocessimport uuidimport requestsfrom multiprocessing.dummy import Poolvideo_url = &apos;&apos;url = &apos; &apos;filename = url.split(&apos;/&apos;)[-1]mp4_name = filename.split(&apos;.&apos;)[0]# 总想把前2个方法合成一个，目前是不行的。def get_m3u8File(url): res = requests.get(url).content with open(filename, &apos;wb&apos;) as f: f.write(res)def get_tsFile(): for i in open(filename, &apos;r&apos;): if i.strip()[0] != &apos;#&apos;: # ts_url = &apos; &apos; + i.strip() ts_url = &apos; &apos; + i.strip() # print(ts_url) # yield ts_url ts = requests.get(ts_url).content with open(ts_url.split(&apos;/&apos;)[-1], &apos;wb&apos;) as g: g.write(ts)if __name__ == &apos;__main__&apos;: # get_m3u8File(url) # get_tsFile() # print(&quot;got all ts !&quot;) # cmd1 = &apos;copy /b *.ts &#123;&#125;.ts &apos;.format(mp4_name) cmd2 = &apos;ffmpeg -i &#123;0&#125;.ts -vcodec copy -acodec copy &#123;1&#125;.mp4 &apos;.format(mp4_name, mp4_name) cmd3 = &apos;del *.ts&apos; cmd4 = &apos;del *.m3u8&apos; # subprocess.call(cmd1, shell=True) # subprocess.call(cmd2, shell=True) # subprocess.call(cmd3, shell=True) subprocess.call(cmd4, shell=True) # print(&apos;got mp4&apos;)","categories":[],"tags":[]},{"title":"505今日头条","slug":"505今日头条","date":"2018-05-05T15:22:10.000Z","updated":"2018-05-05T15:29:19.247Z","comments":true,"path":"2018/05/05/505今日头条/","link":"","permalink":"http://buxuele.github.io/2018/05/05/505今日头条/","excerpt":"","text":"下载今日头条图片（根据首页搜索结果）下一步是根据用户主页来写 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364# -*- coding: utf-8 -*-# author: fanchuangwater Administrator# time: 2018/5/5 18:12import timeimport osimport requestsfrom multiprocessing.dummy import Pool&apos;&apos;&apos;0. 直接在内存中进行。不要再写入额外的文件了。1. 这个文件是再次测试 第一个版本的，试试 关键词为：“上海”2. 下一步是根据 某用户的主页来进行 &apos;&apos;&apos;headers = &#123; &apos;cookie&apos;: &apos;tt_webid=6547250260123731469; WEATHER_CITY=%E5%8C%97%E4%BA%AC; UM_distinctid=162ed599746a00-037e0ac2fce924-3e3d510f-1fa400-162ed599747d46; _ga=GA1.2.844640822.1524400547; uuid=&quot;w:b3c29a34829040c0886a87fa70db15c6&quot;; tt_webid=6547250260123731469; __utma=24953151.844640822.1524400547.1525496653.1525496653.1; __utmz=24953151.1525496653.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none); CNZZDATA1259612802=246391495-1524398248-https%253A%252F%252Fwww.google.com.hk%252F%7C1525500131; __tasessionId=1tf0k2dqa1525503020866&apos;, &apos;user-agent&apos;: &apos;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.168 Safari/537.36&apos;&#125;kw = input(&apos;请输入关键词：&apos;)s = requests.Session()# 1.给1页（1个url）, 拿到x个内容def getPics(): picLinks = [] for r in range(9): u = &apos;https://www.toutiao.com/search_content/?offset=&apos; + str( r * 20) + &apos;&amp;format=json&amp;keyword=%s&amp;autoload=true&amp;count=20&amp;cur_tab=3&amp;from=gallery&apos; % kw wbdata = s.get(u, headers=headers) data_json = wbdata.json() data_20 = data_json[&apos;data&apos;] for i in data_20: if &apos;title&apos; in i: for p in i[&apos;image_list&apos;]: # line = &apos;http:&apos; + p[&apos;url&apos;].replace(&apos;list&apos;, &apos;origin&apos;) + &apos;\\n&apos; picLinks.append(&apos;http:&apos; + p[&apos;url&apos;].replace(&apos;list&apos;, &apos;origin&apos;)) return picLinksdef download(url): print(&quot;正在下载：&quot; + url) pic = s.get(url).content fn = url.split(&apos;/&apos;)[-1] + &apos;.jpg&apos; with open(fn, &apos;wb&apos;) as f: f.write(pic)if __name__ == &apos;__main__&apos;: begin = time.time() os.makedirs(kw, exist_ok=True) os.chdir(kw) ok = getPics() my_pool = Pool(20) my_pool.map(download, ok) my_pool.close() my_pool.join() print(&quot;下载完成了！总共消耗的时间是：&quot; + str(time.time() - begin))","categories":[],"tags":[]},{"title":"505读书笔记03","slug":"505读书笔记03","date":"2018-05-05T06:00:41.000Z","updated":"2018-05-05T06:16:19.699Z","comments":true,"path":"2018/05/05/505读书笔记03/","link":"","permalink":"http://buxuele.github.io/2018/05/05/505读书笔记03/","excerpt":"","text":"2件事，1是爬了一些nba 图片，2是决定以前的笔记不再管了，只是搞好现在的。 读书笔记031. 据说有心理学研究发现，商业白领的性能力与其工作业绩相关。 2. 一个苏格兰人送给一个妇女一只口红，他说：反正我能把口红要回来。 3. 君子怕小人，活人怕死鬼，男人怕老婆。 4. 跑步时，当你的身体从有氧状态转变为无氧状态时，虽然肌肉和细胞急需大量氧气，可身体却是在没有足够氧气下运转了，这时候“跑步者的愉悦感”就出现了。 5. 残篇123：事物的本质喜欢隐藏起来。 6. 中国传统文化的实质是一种伦理型的文化。 7.《春秋》的大旨着重揭露人的用心，凡事苛求动机。 8. 朱熹：世上无如人欲险，几人到此误平生。 9. 拈毫一字几踌躇。 10. 1英里=1.609公里，1英尺=30.48厘米， 1英寸=2.54厘米 11. 说谎的人都希望搭上诚实的便车，也就是希望别人都诚实，自然也就不会有人疑心他是在说谎。","categories":[],"tags":[]},{"title":"430_west_world_season_2","slug":"430-west-world-season-2","date":"2018-04-30T14:54:12.000Z","updated":"2018-04-30T14:57:31.494Z","comments":true,"path":"2018/04/30/430-west-world-season-2/","link":"","permalink":"http://buxuele.github.io/2018/04/30/430-west-world-season-2/","excerpt":"","text":"west world s2e2","categories":[],"tags":[]},{"title":"430zhiboba","slug":"430zhiboba","date":"2018-04-29T16:07:23.000Z","updated":"2018-04-29T16:10:02.438Z","comments":true,"path":"2018/04/30/430zhiboba/","link":"","permalink":"http://buxuele.github.io/2018/04/30/430zhiboba/","excerpt":"","text":"尝试下载直播吧的图片 ，还需要改进 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101# -*- coding: utf-8 -*-# author: fanchuangwater Administrator# time: 2018/4/29 21:15# about: 尝试下载直播吧的图片# 这个 版本还需要再改进。。。import reimport timeimport osimport requestsfrom bs4 import BeautifulSoupfrom multiprocessing.dummy import Poolfrom fake_useragent import UserAgentu = UserAgent()# url = &apos;https://tu.zhibo8.cc/home/album/40517/&apos;headers = &#123; &apos;User-Agent&apos;: u.random&#125;start_url = &apos;https://tu.zhibo8.cc/zuqiu/all/3&apos;# 访问一个网页 ，返回 responsedef ask_page(url): s = requests.Session() page = s.get(url, headers=headers) # print(page.status_code) # print(page.encoding) return page# 找到网页中想要的链接，并返回def get_link(url): page = ask_page(url) soup = BeautifulSoup(page.text, &apos;lxml&apos;) lis = soup.find_all(src=re.compile(&apos;imgcdn.zhibo8.cc&apos;)) print(len(lis)) # 17 for l in lis[1: -6]: # print(l) # print(l.get(&apos;src&apos;)) u = &apos;https:&apos; + l.get(&apos;src&apos;)[: -10] + l.get(&apos;src&apos;)[-4:] # print(u) yield u# 下载图片def download(url): print(&quot;正在下载：&quot; + url) pic = ask_page(url).content # 获取图片的内容 fn = url.split(&apos;/&apos;)[-1] with open(fn, &apos;wb&apos;) as f: f.write(pic)#def feed(start_url): full_pages = [] bo = ask_page(start_url) soup = BeautifulSoup(bo.text, &apos;lxml&apos;) ss = soup.find_all(href=re.compile(r&apos;/home/album/&apos;)) sss = ss[: -11] for s in range(len(sss)): if s % 2 == 0: # print(s.get(&apos;href&apos;)) print(sss[s].get(&apos;href&apos;)) full_page = &apos;https://tu.zhibo8.cc&apos; + sss[s].get(&apos;href&apos;) full_pages.append(full_page) return full_pages# 多线程下载if __name__ == &apos;__main__&apos;: # os.makedirs(&apos;z2&apos;, exist_ok=True) os.chdir(&apos;z2&apos;) start = time.time() for url in feed(start_url): lse = get_link(url) pool = Pool(20) pool.map(download, lse) pool.close() pool.join() end = time.time() print(&quot;下载完成！下载一共消耗的时间是：&quot; + str(end - start)) # ask_page(url) # get_link(url) # feed(start_url)","categories":[],"tags":[]},{"title":"卫国勇士","slug":"卫国勇士","date":"2018-04-28T16:55:45.000Z","updated":"2018-04-28T16:59:17.553Z","comments":true,"path":"2018/04/29/卫国勇士/","link":"","permalink":"http://buxuele.github.io/2018/04/29/卫国勇士/","excerpt":"","text":"女主角。","categories":[],"tags":[]},{"title":"423rain","slug":"423rain","date":"2018-04-23T13:10:41.000Z","updated":"2018-04-23T13:14:43.083Z","comments":true,"path":"2018/04/23/423rain/","link":"","permalink":"http://buxuele.github.io/2018/04/23/423rain/","excerpt":"","text":"下雨了。对于发芽的种子是个好事。","categories":[],"tags":[]},{"title":"plant01","slug":"422plant01","date":"2018-04-22T15:05:35.000Z","updated":"2018-04-23T13:14:45.075Z","comments":true,"path":"2018/04/22/422plant01/","link":"","permalink":"http://buxuele.github.io/2018/04/22/422plant01/","excerpt":"","text":"阳台的花盆，放了一些种子。发芽了。","categories":[],"tags":[]},{"title":"rename_files_with_python","slug":"rename-files-with-python","date":"2018-04-21T14:12:36.000Z","updated":"2018-04-21T14:14:48.018Z","comments":true,"path":"2018/04/21/rename-files-with-python/","link":"","permalink":"http://buxuele.github.io/2018/04/21/rename-files-with-python/","excerpt":"","text":"借鉴了别人的写法，稍微改下：123456789101112131415161718192021222324252627282930313233 # -*- coding: utf-8 -*-# author: fanchuangwater Administrator# time: 2018/4/21 20:28# description: 批量重命名文件import ospath_name = input(&quot;Your file path: &quot;)def rename_files(path_name): i = 0 all_file = os.listdir(path_name) for a in all_file: print(a) old_file = os.path.join(path_name, a) print(old_file) b = str(i) + a[-4:] print(b) new_file = os.path.join(path_name, b) print(new_file) os.rename(old_file, new_file) print(&apos;***************&apos;) i = i + 1rename_files(path_name)","categories":[],"tags":[]},{"title":"pic_test","slug":"pic-test","date":"2018-04-21T11:14:42.000Z","updated":"2018-04-22T15:15:29.788Z","comments":true,"path":"2018/04/21/pic-test/","link":"","permalink":"http://buxuele.github.io/2018/04/21/pic-test/","excerpt":"","text":"test some thing about markdown 插入网络图片测试01 本地图片测试02","categories":[],"tags":[]}]}